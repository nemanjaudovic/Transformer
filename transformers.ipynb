{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uvod\n",
    "\n",
    "Nama relevantan primer je primer prevodioca, i to za početak sa srpskog jezika na srpski znakovni jezik. S obzirom da takva baza ne postoji, ovde ću koristiti neki generic primer, ali ću objašnjavati šta bi trebalo da se dešava u našem slučaju. \n",
    "\n",
    "S obzirom da ja paralelno učim i pišem ovo, probaću da objašnjavam tačno šta radi python kod nezavisno od konteksta transformera.\n",
    "\n",
    "Takođe, probaću da sve što je moguće i teorijski objasnim. Znanje neuralnih mreža je korisno, ali potrudiću se da ne bude neophodno.\n",
    "\n",
    "To znači da svako parče koda ima 3 dela:\n",
    "\n",
    "1. Sve nove komande koje do sad nisu viđene, a korisno je da se zna šta rade generalno;\n",
    "\n",
    "2. Šta kod postiže;\n",
    "\n",
    "3. Teoretska osnova za taj deo.\n",
    "\n",
    "Ispod je napisano sve neophodno da se kod izvršava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset as TorchDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import Dataset as HFDataset\n",
    "from translate.storage.tmx import tmxfile\n",
    "\n",
    "import warnings\n",
    "import torchmetrics\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import math\n",
    "import tabulate\n",
    "\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Transformer je arhitektura za deep learning (gde deep znači da se koristi više slojeva u mreži, a sve se radi na neuralnim mrežama), koja je dizajnirana u radu [Attention is all you need](https://arxiv.org/pdf/1706.03762). Sastoji se od encoder-a i decoder-a. \n",
    "\n",
    "Tekst se konvertuje u tokene, koji se zatim konvertuju u vektore (što se zove embedding), u nekom višedimenzionom vektorskom prostoru. Početni embedding je klasičan dictionary, gde svaki token iz rečnika ima svoj embedding.\n",
    "\n",
    "U svakom sloju, suština je da svaki embedding primi kontekst od drugih tokena, i samim tim update-uje vrednosti za svoj embedding. Ovime se token u ovom vektorskom prostoru sve više i više približava svojoj najprigodnijoj lokaciji.\n",
    "\n",
    "U nastavku pratimo šta se dešava prilikom treninga transformera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baza podataka\n",
    "\n",
    "Za početak, moramo da definišemo bazu podataka sa kojom će naš model da radi. Ona se nadovezuje na `torch` bazu, i implementira njene tri glavne metode: `__init__(dataset, source_tokenizer, target_tokenizer, source_language, target_language, context_size)`, `__len__()` i `__getitem__(index)`, koje redom inicijalizuju objekat, vraćaju njegovu veličinu (to jest broj podataka - za nas parova rečenica) i vraćaju konkretan podatak sa datog indeksa.\n",
    "\n",
    "Uz to, unapred moramo da definišemo još jednu funkciju, a koja će biti objašnjena kasnije. To je funkcija `causal_mask(size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size: int) -> torch.Tensor:\n",
    "    \n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "class BilingualDataset(TorchDataset):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: HFDataset, \n",
    "            source_tokenizer: Tokenizer, \n",
    "            target_tokenizer: Tokenizer, \n",
    "            source_language: str, \n",
    "            target_language: str, \n",
    "            context_size: int\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "\n",
    "        self.source_language = source_language\n",
    "        self.target_language = target_language\n",
    "        \n",
    "        self.sos_token = torch.tensor([source_tokenizer.token_to_id('[SOS]')], dtype = torch.int64)\n",
    "        self.eos_token = torch.tensor([source_tokenizer.token_to_id('[EOS]')], dtype = torch.int64)\n",
    "        self.pad_token = torch.tensor([source_tokenizer.token_to_id('[PAD]')], dtype = torch.int64)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    \n",
    "    def __getitem__(\n",
    "            self, \n",
    "            index: int\n",
    "        ) -> Dict[str, Any]:\n",
    "        source_target_pair = self.dataset[index]\n",
    "\n",
    "        source_text = source_target_pair['translation'][self.source_language]\n",
    "        target_text = source_target_pair['translation'][self.target_language]\n",
    "\n",
    "        encoder_input_tokens = self.source_tokenizer.encode(source_text).ids\n",
    "        decoder_input_tokens = self.target_tokenizer.encode(target_text).ids\n",
    "\n",
    "        encoder_num_padding_tokens = self.context_size - len(encoder_input_tokens) - 2\n",
    "        decoder_num_padding_tokens = self.context_size - len(decoder_input_tokens) - 1\n",
    "        \n",
    "        if encoder_num_padding_tokens < 0 or decoder_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long!\")\n",
    "        \n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(encoder_input_tokens, dtype = torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * encoder_num_padding_tokens, dtype = torch.int64)\n",
    "            ],\n",
    "            dim = 0\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(decoder_input_tokens, dtype = torch.int64),\n",
    "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype = torch.int64)\n",
    "            ],\n",
    "            dim = 0\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(decoder_input_tokens, dtype = torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype = torch.int64)\n",
    "            ],\n",
    "            dim = 0\n",
    "        )\n",
    "\n",
    "        assert encoder_input.size(0) == self.context_size\n",
    "        assert decoder_input.size(0) == self.context_size\n",
    "        assert label.size(0) == self.context_size\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"source_text\" : source_text,\n",
    "            \"target_text\" : target_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primećujemo da vraćanje elementa iz baze vraća više stvari. Sve one će biti objašnjene naknadno. To su:\n",
    "\n",
    "1. encoder_input: Ulaz u encoder modela;\n",
    "\n",
    "2. decoder_input: Ulaz u decoder modela;\n",
    "\n",
    "3. encoder_mask: Maska za encoder;\n",
    "\n",
    "4. decoder_mask: Maska za decoder;\n",
    "\n",
    "5. label: Očekivani izlaz iz transformer-a;\n",
    "\n",
    "6. source_text: Rečenica na originalnom jeziku;\n",
    "\n",
    "7. target_text: Prevedena rečenica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Encoder je deo transformer-a koji se odnosi na (u slučaju prevodioca) procesiranje ulazne rečenice. Isključivo u ovom delu će model naučiti (nadamo se) neku formu jezika i uspeti da razume odnose između reči."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Cilj prevodioca je da primi rečenicu na jednom jeziku i izbaci istu tu rečenicu na drugom jeziku. Za nas je rečenica uređena lista reči, a za kompjuter - niz tokena. Tokeni su, u generalnom slučaju, osnovni delovi građe teksta. Ne moraju biti reči u ljudskom shvatanju, iako se mogu i tako postaviti. \n",
    "\n",
    "U nekoj arhitekturi je moguće smatrati da je reč \"obaveštavamo\" jedan token, a u drugoj bi to mogla biti i dva tokena \"obaveštava\" + \"mo\". Prednosti postoje za obe situacije, a posmatra se ušteda vremena i mogućnost boljeg razumevanja teksta.\n",
    "\n",
    "Međutim, nisu reči jedini sastavni deo teksta. Postoje i neke stvari koje ljudi uzimaju zdravo za gotovo, a to su definitivno početak i kraj rečenice, interpukcijski znakovi, činjenica da neke reči imaju različito značenje u zavisnosti od toga da li im je prvo slovo veliko ili malo, i tako dalje. Sve ovo je neophodno uzeti u obzir.\n",
    "\n",
    "Dakle, prvi korak je prebaciti tekst koji želimo u tokene.\n",
    "\n",
    "Tokenizer je deo programa koji listu svih rečenica koje imamo u bazi (dakle onih sa kojima naš transformer treba da radi) pretvara u rečnik tokena. Svakom tokenu koji se nađe u ovim rečenicama dodeljuje jedinstveni id.\n",
    "\n",
    "Takođe, postoje $4$ specijalna tokena, a to su:\n",
    "\n",
    "1. ***[UNK]*** token koji će (u korišćenju i treniranju modela) menjati one tokene koji nemaju mesto u rečniku. To su ili tokeni kojih nema u bazi, ili oni sa nedovoljnim brojem pojavljivanja.\n",
    "\n",
    "2. ***[PAD]*** token koji služi da rečenicu koja nije dovoljno dugačka dopuni do kraja. Transformer radi sa rečenicama konstantne dužine (u tokenima), pa da bi bio upotrebljiviji, ukoliko ima manje tokena od te dužine, dopuni se ovim tokenima.\n",
    "\n",
    "3. ***[SOS]*** i ***[EOS]*** tokeni koji označavaju početak i kraj rečenice.\n",
    "\n",
    "Dakle, svaka rečenica će nakon tokenizer-a da ima izgled:\n",
    "\n",
    "$$\\left[\\text{SOS}\\right]\\ T_1\\ T_2\\ T_3\\ \\dots\\ T_K\\ \\left[\\text{EOS}\\right]\\ \\left[\\text{PAD}\\right]\\ \\left[\\text{PAD}\\right]\\ \\dots\\ \\left[\\text{PAD}\\right]$$\n",
    "\n",
    "gde će ukupan broj tokena uvek da bude isti, i zvaćemo ga `context_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre definicije tokenizer-a, treba nam i fajl koji će čuvati hiperparametre modela, kao i neke njegove opisne karakteristike (jezici koji se koriste, ime modela, ...). To radimo sa `config`-om, koji pozivamo funkcijom `get_config()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epochs\": 100,\n",
    "        \"learning_rate\": 3 * 10**-4,\n",
    "        \"context_size\": 64,\n",
    "        \"model_dimension\": 128,\n",
    "        \"source_language\": \"en\",\n",
    "        \"target_language\": \"asl\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"english_to_gloss_\",\n",
    "        \"preload\": None,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/english_to_gloss\",\n",
    "        \"seed\": 561\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada možemo da pređemo i na učitavanje tokenizer-a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(\n",
    "        dataset: HFDataset,\n",
    "        language: str\n",
    "    ):\n",
    "    for item in dataset:\n",
    "        yield item['translation'][language]\n",
    "\n",
    "def get_or_build_tokenizer(\n",
    "        config, \n",
    "        dataset: HFDataset, \n",
    "        language: str,\n",
    "        force_rewrite: bool = False,\n",
    "        min_frequency: int = 5,\n",
    "        vocab_size: int = 1000000\n",
    "    ) -> Tokenizer:\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(language))\n",
    "\n",
    "    if not Path.exists(tokenizer_path) or force_rewrite:\n",
    "\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        trainer = WordLevelTrainer(\n",
    "            special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], \n",
    "            min_frequency = min_frequency, \n",
    "            vocab_size = vocab_size\n",
    "            )\n",
    "        \n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer = trainer)\n",
    "\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "    else: \n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    print(f\"Number of tokens in {language} is {tokenizer.get_vocab_size()}.\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer koji je napravljen u kodu za tokene ima reči i interpukcijske znakove. Ukoliko je moguće, biće učitan iz odgovarajućeg .json fajla, a u suprotnom, rečenice će biti procesuirane jedna po jedna, a na kraju, svi tokeni koji imaju bar `min_frequency` pojavljivanja u njima, će biti ubačeni u rečnik. Rečnik o kome ovde govorimo ima za key-eve tokene a za value-e ima int-ove koji se ponašaju kao redni broj tokena u rečniku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input embeddings\n",
    "\n",
    "Nakon dobijanja liste tokena, želimo da dobijemo i njihove vektorske reprezentacije - embeddinge.\n",
    "\n",
    "Embedding je samo jedan element nekog ogromno-dimenzionog vektorskog prostora. U pomenutom radu koristi se $512$ dimenzija, u ChatGPT 3 se koristi $12288$. Ovaj broj ćemo označavati sa $d_{model}$.\n",
    "\n",
    "Razlog za veliki broj dimenzija je ogroman broj reči i mogućnosti za njihovo značenje. Dobar primer za ovo je što, u slučaju GPT-a, jedan pravac u ovom $12288$-dimenzionom prostoru označava pol. Zapravo, ako se posmatra razlika vektora kojima odgovaraju muškarac i žena, i ta razlika se sabere sa nekom imenicom ženskog pola, dobiće se približno vektor odgovarajuće imenice muškog pola (*kraljica* $\\rightarrow$ *kralj*, *majka* $\\rightarrow$ *otac*, itd). Slično, postoje smerovi koji odgovaraju množini, državljanstvu, i sličnim pojmovima. Ovo se naravno odnosi na već istreniran transformer. Naravno da je ovakva interpretabilnost retkost i da su pravci u vektorskom prostoru u velikom broju slučajeva neki pattern-i koji za ljude nemaju smisla.\n",
    "\n",
    "Takođe, slični tokeni (po značenju) se nalaze blizu jedan drugog po vektorima. Ovo je još jedan razlog zašto je veliki broj dimenzija bitan, jer ima više prostora da se slične stvari nađu bliže međusobno nego sa ostalim, dok još preostaje prostora da se bitno razlikuju i međusobno. Da postoje, na primer, tri dimenzije, praktično ne bi bilo dovoljno prostora da se tako nešto izvede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_dimension: int, \n",
    "            vocab_size: int\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dimension)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.embedding(x) * math.sqrt(self.model_dimension)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "Nije dovoljno da se modelu predaju jedino embeddings za sve tokene. Njihov kontekst zavisi i od pozicije u rečenici. \n",
    "\n",
    "Positional encoding je standardan način da se preda i ta informacija. Ovo ne zavisi od modela i od tokena, već samo od pozicije i veličine embeddinga.\n",
    "\n",
    "Formule koje se koriste su\n",
    "$$PE(\\text{position}, 2i) = \\sin \\frac{\\text{position}}{10000 ^ {\\frac{2i}{d_{model}}}},$$\n",
    "za parne pozicije u vektoru, i\n",
    "$$PE(\\text{position}, 2i + 1) = \\cos \\frac{\\text{position}}{10000 ^ {\\frac{2i}{d_{model}}}},$$\n",
    "za neparne.\n",
    "\n",
    "Ovde se koriste neke bitne funkcije iz `pytorch`-a. To su:\n",
    "\n",
    "1. `torch.arange(start, end)` - Slična kao standardni python range, uz to što sada vraća jednodimenzionalni tenzor. (Tenzor je generalizacija vektora i matrica na više dimenzija. 1D tenzor je matematički isto što i vektor.)\n",
    "\n",
    "2. `torch.zeros(*size)` - Tenzor sa svim nulama i zadatim dimenzijama.\n",
    "\n",
    "3. `torch.tensor.unsqueeze(n)` - Ubacuje još jednu dimenziju u tenzor, na mesto prosleđenog parametra. Ako smo imali tenzor dimenzije $\\left(a_0 \\times a_1 \\times ... \\times a_{j - 1}\\right)$, onda ovo ubacuje dimenziju (veličine 1) na n-to mesto u ovom nizu, pa tenzor postaje dimenzije $\\left(a_0 \\times a_1 \\times ... \\times a_{n - 2} \\times 1 \\times a_{n - 1} \\times ... \\times a_{j - 1}\\right).$\n",
    "\n",
    "4. Sve matematičke funkcije u `torch`-u se primenjuju na svaki element tenzora zasebno, i vraća se tenzor iste dimenzije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_dimension: int, \n",
    "            context_size: int, \n",
    "            dropout: float\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "        self.context_size = context_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        positional_encodings = torch.zeros(context_size, model_dimension)\n",
    "        position = torch.arange(0, context_size, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dimension, 2).float() * (-math.log(10000.0) / model_dimension))\n",
    "        \n",
    "        positional_encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        positional_encodings = positional_encodings.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', positional_encodings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par bitnih stvari vezanih za kod su sledeće:\n",
    "1. `register_buffer(name, tensor)` je metod u torch-u koji suštinski postavlja parametre za model koji se neće trenirati. U ovom slučaju, `pe` je samo positional encoding, koji je konstantan.\n",
    "\n",
    "2. `requires_grad_(bool)` omogućava pamćenje operacija koje su učinjene nad nekim tenzorom, dok ga postavljanje na `False` tera da ih ne uzima u obzir nikad.\n",
    "\n",
    "Ovde uvek vraćamo samo statičku vrednost za positional encoding, koja se dodaje na već napravljeni input embedding. Ovime dobijamo kodirane vrednosti svih tokena, zajedno sa svojim originalnim značenjem i pozicijom u rečenici."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacija\n",
    "\n",
    "Korak koji će često biti korišćen je normalizacija. Čisto služi kao metod za smirivanje podataka, ne razlikuje se mnogo od standardnih normalizacija.\n",
    "\n",
    "U ovom slučaju, svaki vektor se normalizuje zasebno (u odnosu na svoju srednju vrednost i disperziju). Pričam ovde vektor jer se radi ne samo na početnim tokenima, već u praktično svakom koraku transformera.\n",
    "\n",
    "Ukoliko imamo vektor $\\left(a_0, a_1, \\dots, a_{d_{model} - 1}\\right)$, sa srednjom vrednošću $\\mu$ i disperzijom $\\sigma^2$, normalizacija koja se radi je\n",
    "$$\\hat{x} = \\alpha \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta.$$\n",
    "\n",
    "Parametri $\\alpha$ i $\\beta$ služe za ubacivanje podataka u odgovarajući, željeni interval. Parametar $\\varepsilon$ služi za numeričku stabilnost. Omogućava da čak iako su vrednosti $\\sigma^2$ male, ne dobijemo grešku deljenja sa nulom (što se dešava kada su svi podaci približno isti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            eps: float = 10**-6\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U kodu `bias` igra ulogu prethodno definisanog parametra $\\beta$. Inicijalizacija ovih promenljivih kao `nn.Parameter(data)` omogućava da se treniraju.\n",
    "\n",
    "Sam kod je ništa više nego implementacija formule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Ovo je glavni i najzanimljiviji korak celog transformera. Deo je i enkodera i dekodera, ali na malo drugačiji način, iako je logika ista.\n",
    "\n",
    "Naziv attention označava (generalno u ML-u) to da se iz podataka izvlače samo bitni, relevantni podaci. Postiže se tako što model uči weight-ove koji daju različit značaj različitim elementima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single-head self attention\n",
    "\n",
    "Ovaj pojam je u srcu celog attention bloka. Fokusiramo se na primer. Nakon što se mreža stvarno istrenira, teško da postoji mogućnost da čovek razume šta ti weight-ovi koji su dobijeni stvarno označavaju, ali kroz primer će se videti šta planiramo da postignemo, a kompjuter će sam izvesti svoje razumevanje toga.\n",
    "\n",
    "Pravimo prevodioca, tako da pretpostavljamo da imamo neku rečenicu na srpskom jeziku koju želimo da prevedemo na srpski znakovni jezik. Neka je ta rečenica ***Brzi voz 271 dolazi na kolosek broj 7.***\n",
    "\n",
    "U ovoj rečenici postoje dve glavne imenice, voz i kolosek. Pretpostavimo da je željeni rezultat da vidimo koje sve druge reči bliže određuju (zasebno) te imenice. (Takođe, pravimo se da su tokeni reči.) Mi znamo da su to **brzi** za voz i **broj 7** za kolosek.\n",
    "\n",
    "S obzirom da se bavimo ML-om, jedini način da nešto ovako proverimo je nekakvim operacijama nad matricama i vektorima.\n",
    "\n",
    "To znači da trenutni embedding za reči *voz* i *brzi* moraju da imaju način da komuniciraju. Reč *voz* kao da pita \"Šta me bliže određuje?\", dok *brzi* odgovara \"Ja te bliže određujem!\". \n",
    "\n",
    "Ukoliko postoji način da kodiramo to pitanje i odgovor u istodimenzionom vektorskom prostoru, način da se proveri koliko su oni bliski je njihob skalarni proizvod. Što je on veći, odnosno manji, to su vektori bliskiji, odnosno različitiji.\n",
    "\n",
    "Razlog zašto posmatramo skalarne proizvode neskaliranih vektora je to što hoćemo da njihova dužina zapravo utiče na rezultat. Intuitivno, reči kao *katastrofa*, koje bi imale veliku dužinu kao vektori, i treba da puno utiču na ostale tokene zbog njihove prirode. Intenzitet vektora je, dakle, način da predstavimo intenzitet reči.\n",
    "\n",
    "Ostaje pitanje kako dobijamo te vektore, kojima postavljamo pitanje i dajemo odgovor. Naravno, množimo trenutne embedding-e nekim matricama.\n",
    "\n",
    "Proces koji se dešava je sledeći:\n",
    "\n",
    "1. Imamo trenutne embedding-e (koji uključuju i poziciju u rečenici i originalno značenje reči), koje označavamo sa $E_0, E_1, \\dots, E_{N-1}$. To su vektori dužine $d_{model}$. Svi oni su raspoređeni u jednu matricu veličine $N \\times d_{model},$ koju ćemo nazivati $I$, a gde su zasebne reči redovi u matrici ($I$ je od Input, jedinična matrica se označava sa $E_n$). U zavisnosti od konteksta, označavaćemo je sa još $Q, K, V$, kada se govori o query-ju, key-u, ili value-u (što ćemo videti šta označava).\n",
    "\n",
    "2. Imamo neke matrice $W_Q$, i $W_K$ (koje su parametri modela), dimenzija (u opštem slučaju) $d_{model} \\times d_{KQ}$, gde je, u slučaju ovog koda $d_{model} = d_{KQ}$, ali generalno je dozvoljeno da se razlikuje. (U slučaju ChatGPT-a je veličine $128$.)\n",
    "\n",
    "3. Množenjem matrica $Q \\times W_Q$ i $K \\times W_K$ dobijamo matrice koje označavamo redom sa $Q'$ i $K'$, i koje imaju dimenziju $N \\times d_{KQ}$. (Ovde se vidi razlika između encoder-a i decoder-a. U encoder-u, $Q$ i $K$ su iste matrice, dok ćemo u decoder-u dozvoliti da budu različite.)\n",
    "\n",
    "4. Naravno, $k$-ti red tih dobijenih matrica se odnosi na $k$-tu reč u rečenici. Da bismo našli skalarni proizvod koji nas zanima, uzimamo $2.$ red iz $Q'$, i $1.$ red iz $K'$. Ovo se postiže takođe i uzimanjem proizvoda $Q'K'^{T}$, čime se dobija matrica dimenzija $N \\times N$. Na $(k, j)$ poziciji u ovoj matrici nalazi se skalarni proizvod $k$-tog reda iz $Q'$ i $j$-tog reda iz $Q'$.\n",
    "\n",
    "U gornjim koracima, broj $N$ označava `context_size`.\n",
    "\n",
    "Sada imamo nešto što označava kako reči međusobno utiču. Pitanje je kako da sad da tom informacijom prođemo kroz reči, i napravimo ih reprezentativnijim za dati kontekst. Za to služi Value matrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "Treba da prethodno dobijenu matricu skaliramo tako da dobijemo verovatnoće. Za to koristimo standardnu funkciju **softmax**. Ova funkcija pretvara neki vektor realnih brojeva u funkciju raspodele. Važi:\n",
    "$$\\text{softmax}\\left(a_1, a_2, \\dots, a_n\\right)=\\frac{1}{\\displaystyle\\sum_{i=1}^n e^{a_i}}\\left(e^{a_1}, e^{a_2}, \\dots, e^{a_n}\\right).$$\n",
    "Ovime dobijamo brojeve veće od nula, a koji se sabiraju do $1$. Primećujemo da postavljanje nekog od $a_i$ na $-\\infty$ odgovara tome da će dobijeni vektor na toj poziciji da sadrži nulu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value matrica\n",
    "\n",
    "Da bismo dobili rezultat attention-a, izvodimo sledeće korake:\n",
    "\n",
    "1. Matrica $Q'K'^{T}$ se skalira brojem $\\frac{1}{\\sqrt{d_k}}$ (u slučaju single head attention-a, broj $d_k$ je isti kao i $d_{model}$, dok se u opštem slučaju računa kao $d_{model} / h$), a zatim na dobijenu matricu primeni funkcija softmax (na redove te matrice!)\n",
    "\n",
    "2. Imamo matricu $W_V$ (takođe parametar modela) i dimenzije $d_{model} \\times d_{model}$. \n",
    "\n",
    "3. Množimo matrice $V$ i $W_V$ (koja je ista kao $I$ u slučaju encoder-a), čime dobijamo $V'$, matricu dimenzije $N \\times d_{model}$.\n",
    "\n",
    "4. Množimo rezultat prvog koraka (matrica skalirana a zatim primenjen softmax) i $V'$. Ovime dobijamo i rezultat single head attention-a, matricu veličine $N \\times d_{model}.$\n",
    "\n",
    "Rezultat je matrica iste veličine kao i ona sa kojom smo počeli. Praktično, rezultat se tumači kao novi embedding-ovi, koji su primili kontekst od ostalih reči."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking u encoder-u\n",
    "\n",
    "Videli smo malopre da, ako je, na primer, $\\text{context size} = 16$, rečenica ima oblik ***[SOS] Brzi voz 271 dolazi na kolosek broj 7 [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]***.\n",
    "\n",
    "Mi ne želimo da token ***[PAD]*** utiče na reči, jer on zapravo ne postoji, i služi samo da bismo rečenicu dopunili do odgovarajuće dužine.\n",
    "\n",
    "To postižemo maskiranjem, koje u ovom slučaju zovemo *padded masking*. Rezultate u matrici $Q'K'^{T}$ koji odgovaraju ovim ***[PAD]*** tokenima menjamo sa $-\\infty$. Time postižemo da oni ne utiču na krajnji rezultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head self attention\n",
    "\n",
    "Umesto da posmatramo potpune embedding-e za svaki vektor, možemo njih da podelimo na $h$ delova (mora da važi $h \\mid d_{model}$, i broj $\\frac{d_{model}}{h}$ označavamo sa $d_k$, ili $\\text{head dimension}$). \n",
    "\n",
    "Ovo deljenje postižemo projekcijom matrica $Q, K, V$ na $h$ dimenzija, i to na sledeći način:\n",
    "\n",
    "$$Q_i = QW_i^Q$$\n",
    "$$K_i = KW_i^K$$\n",
    "$$V_i = VW_i^V$$\n",
    "\n",
    "Svaka glava potom radi single head attention, na potpuno isti način kao malopre:\n",
    "\n",
    "$$\\text{output}_i = \\text{softmax}\\left(\\frac{Q_iK_i^{T}}{d_k}\\right)V_i.$$\n",
    "\n",
    "Matrice $W_i^{Q}$, $W_i^{K}$ i $W_i^{V}$ su sve dimenzija $d_{model} \\times d_k$, što znači da su $Q_i$, $K_i$ i $V_i$ dimenzija $N \\times d_k.$\n",
    "\n",
    "Sada spajamo ove dobijene matrice:\n",
    "\n",
    "$$O = \\text{concat}\\left[\\text{output}_1, \\dots, \\text{output}_h\\right]$$\n",
    "\n",
    "i konačno rezultat množimo matricom $W^O$ ($O$ je dimenzija $N \\times d_{model}$, a $W^O$ dimenzija $d_{model} \\times d_{model}$), i dobijamo matricu koja je konačni rezultat multi head attention-a: $OW^O.$ \n",
    "\n",
    "Primetimo da se broj parametara nije promenio od malopre (osim dodate matrice $W^O$). Razlog zašto se ipak koristi multi head attention je taj što tako omogućavamo raznim rečima da nauče različite mogućnosti za uticaj drugih reči na sebe. Što je više glava, to više raznih načina interakcije između reči model može da nauči. Razni $W_i^{K}$ mogu nezavisno da uče različite stvari, za razliku od jedinstvene matrice u slučaju single head attention-a.\n",
    "\n",
    "Kod je samo implementacija matematičkih formula, uz igranje sa dimenzijama tenzora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_dimension: int, \n",
    "            heads: int, \n",
    "            dropout: float\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        assert model_dimension % heads == 0, \"model_dimension is not divisible by the number of heads.\"\n",
    "\n",
    "        self.head_dimension = model_dimension // heads\n",
    "\n",
    "        self.w_q = nn.Linear(model_dimension, model_dimension)\n",
    "        self.w_k = nn.Linear(model_dimension, model_dimension)\n",
    "        self.w_v = nn.Linear(model_dimension, model_dimension)\n",
    "\n",
    "        self.w_o = nn.Linear(model_dimension, model_dimension)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(\n",
    "            query, \n",
    "            key, \n",
    "            value, \n",
    "            mask, \n",
    "            dropout: nn.Dropout\n",
    "        ):\n",
    "        head_dimension = query.shape[-1]\n",
    "\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(head_dimension)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim = -1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        query = query.view(query.shape[0], query.shape[1], self.heads, self.head_dimension).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.heads, self.head_dimension).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.heads, self.head_dimension).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "         \n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.heads * self.head_dimension)\n",
    "\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward neural network\n",
    "\n",
    "Feed forward je najprostija implementacija neuralne mreže. Naziv označava samo to da podaci idu u jednom smeru (od input-a ka output-u), i da nema drugih veza u mreži.\n",
    "\n",
    "U encoder-u, sastoji se iz samo jednog skrivenog sloja, sa $d_ff$ čvorova. Ona je potpuno povezana. Ulaz u nju je jedan vektor izlaza iz multi-head attention-a, a izlaz je vektor iste te dimenzije. (Dakle, radi posebno sa \"output\" embedding-ovima tokena iz self attention-a!)\n",
    "\n",
    "Njena uloga je (koliko ja razumem) dvostruka:\n",
    "\n",
    "1. Primećujemo da multi-head self attention zapravo samo \"meša\" podatke. Transformacija koja se dobije na kraju tog bloka je samo linearna kombinacija početnih embedding-a. *FNN* je način da unesemo nelinearnost među podatke.\n",
    "\n",
    "2. Omogućavamo da sada token primi kontekst samo od samog sebe, za razliku od ranije, kada je primao kontekst i od svih drugih tokena.\n",
    "\n",
    "Novi modeli su samo dve matrice, redom dimenzija $d_{model} \\times d_{ff}$ i $d_{ff} \\times d_{model}.$\n",
    "\n",
    "Implementacija je još jednom samo primena formule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_dimension: int, \n",
    "            feed_forward_dimension: int, \n",
    "            dropout: float\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(model_dimension, feed_forward_dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(feed_forward_dimension, model_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklapanje encoder-a\n",
    "\n",
    "Sada su tu svi koraci potrebni da bi se sklopio encoder. \n",
    "\n",
    "U kodu je potrebna još jedna stvar. Blokovi u kojima se radi normalizacija zapravo ulaz u prethodni blok sabiraju sa izlazom iz prethodnog bloka za normalizovani ulaz. (U radu \"Attention is all you need\", ovo je implementirano obrnuto, izlaz za normalan ulaz je normalizovan, ali empirijski se pokazalo da obrnuti redosled daje bolje rezultate.)\n",
    "\n",
    "To jest, ako je $I$ ulaz u prethodni blok, a $O(I)$ izlaz iz njega, izlaz iz bloka Add & Norm je $I + O(\\text{norm}(I)).$ (U radu je formula $\\text{norm}(I + O(I))$.)\n",
    "\n",
    "To se postiže na sledeći način:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            dropout: float\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder se zapravo sastoji iz više blokova. Taj broj označavamo sa $N_x$. Jedan blok encoder-a se sastoji iz Self-Attention bloka (zajedno sa svojim Add & Norm blokom), kao i Feed-Forward bloka (zajedno sa svojim Add & Norm) blokom.\n",
    "\n",
    "To znači da se blok encoder-a implementira na sledeći način:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            self_attention_block: MultiHeadAttentionBlock, \n",
    "            feed_forward_block: FeedForwardBlock, \n",
    "            dropout: float\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, source_mask):\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, source_mask))\n",
    "\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sve što je ostalo je da spojimo više encoder blokova u encoder. Treba da spojimo $N_x$ prethodnih blokova u jedan, i normalizujemo izlaz koji dobijemo nakon toga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            layers: nn.ModuleList\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada imamo potpun encoder. Izlaz iz encoder-a je deo ulaza u multi-headed cross attention blok decoder-a, kao što ćemo videti i objasniti u nastavku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "U decoder-u se, za razliku od encoder-a, obrađuje ono što bi trebalo da bude izlaz modela. Služi da procesira tokene jezika na koji hoćemo da prevedemo originalni tekst.\n",
    "\n",
    "Svi blokovi koji se koriste u jednom bloku decoder-a su već definisani. Za razliku od encoder-a, imamo još samo jedan blok, a to je multi-head cross attention.\n",
    "\n",
    "Prvi blok je opet multi-head self attention, zatim multi-head cross attention, i konačno feed forward.\n",
    "\n",
    "Takođe, razlikuje se i ulaz u decoder. U ovom slučaju, ne dodajemo token ***[EOS]*** na kraj rečenice, već je ulaz nešto oblika ***[SOS] BRZI VOZ 271 DOLAZI KOLOSEK 7 [PAD] ... [PAD]***, dok želimo da konačni izlaz iz transformer-a bude ***BRZI VOZ 271 DOLAZI KOLOSEK 7 [EOS] [PAD] ... [PAD]***.\n",
    "\n",
    "Dakle, ulaz u decoder prvo prolazi kroz multi-headed self attention, pa zatim u multi-headed cross attention (zajedno sa izlazima iz encoder-a), i na kraju kroz feed forward. Sve to prati i normalizacija, na isti način kao i u encoder-u.\n",
    "\n",
    "Treba još obratiti pažnju na maskiranje u dekoderu. U cross attention-u ćemo koristiti ponovo *padded masking*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal masking\n",
    "\n",
    "Svrha našeg modela je da prevede rečenicu na drugi jezik, ali koristeći samo rečenicu iz originalnog jezika. Zato je važno da tokeni u decoder-u (to jest, tokeni jezika na koji prevodimo) ne vide tokene koji dolaze nakon njih.\n",
    "\n",
    "To se takođe postiže maskiranjem, baš kao i ranije. I radi se na isti način, samo je sada matrica za maskiranje drugačija. Da bismo poništili dejstvo tokena koji slede, logično je da to treba da bude trougaona matrica.\n",
    "\n",
    "Na početku je bila definisana funkcija `causal_mask(size)`, koja služi da bismo dobili ovu matricu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size: int) -> torch.Tensor:\n",
    "    \n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head cross attention\n",
    "\n",
    "Ono što ovaj deo radi se ne razlikuje od multi-head self attention-a. Jedino što se razlikuje je ulaz u njega.\n",
    "\n",
    "Da se radi o self attention-u, ulaz bi bio izlaz iz prethodnog bloka i to i za $Q$, i za $K$ i za $V$ matrice.\n",
    "\n",
    "Ovde, međutim, to je ulaz samo za $V$. Ulaz za $Q$ i $K$ je izlaz dobijen iz encoder-a. Sva matematika ostaje ista, i ništa se zapravo ne menja. Možemo da koristimo isti blok definisan od ranije."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklapanje decoder-a\n",
    "\n",
    "Slično kao malopre, blok decoder-a se implementira na sledeći način, dok decoder i dalje ima $N_x$ blokova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            self_attention_block: MultiHeadAttentionBlock, \n",
    "            cross_attention_block: MultiHeadAttentionBlock, \n",
    "            feed_forward_block: FeedForwardBlock, \n",
    "            dropout: float\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        \n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        \n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
    "\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, source_mask))\n",
    "\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            layers: nn.ModuleList\n",
    "        )-> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, source_mask, target_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Izlaz iz decoder-a još treba da nekako prebacimo nazad u tokene pa u stringove, jer smo dobili izlaz u obliku embeddinga.\n",
    "\n",
    "To se postiže još jednim linearnim slojem (praktično projekcijom na vektorski prostor), gde je prva dimenzija $d_{model}$ a druga veličina rečnika.\n",
    "\n",
    "Nakon toga, treba da izaberemo koji od tokena će biti sledeći, što opet radimo koristeći **softmax** funkciju, jer dobijeni vektor prvo prebacimo u verovatnoće koristeći je, a zatim odaberemo onu koordinatu sa najvećom verovatnoćom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_dimension: int, \n",
    "            vocab_size: int\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(model_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada imamo sve što je potrebno da napravimo i klasu za transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            encoder: Encoder, \n",
    "            decoder: Decoder, \n",
    "            source_embed: InputEmbeddings, \n",
    "            target_embed: InputEmbeddings, \n",
    "            source_pos: PositionalEncoding, \n",
    "            target_pos: PositionalEncoding, \n",
    "            projection_layer: ProjectionLayer\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embed = source_embed\n",
    "        self.target_embed = target_embed\n",
    "        self.source_pos = source_pos\n",
    "        self.target_pos = target_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        source = self.source_embed(source)\n",
    "        source = self.source_pos(source)\n",
    "        return self.encoder(source, source_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, source_mask, target, target_mask):\n",
    "        target = self.target_embed(target)\n",
    "        target = self.target_pos(target)\n",
    "        return self.decoder(target, encoder_output, source_mask, target_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konačno, treba i da umemo da prosledimo sve parametre. To znači da inicijalizujemo sve parametre modela i napravimo sve neophodne blokove za model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "        source_vocab_size: int, \n",
    "        target_vocab_size: int, \n",
    "        source_context_size: int, \n",
    "        target_context_size: int, \n",
    "        model_dimension: int = 512, \n",
    "        number_of_blocks: int = 6, \n",
    "        heads: int = 8, \n",
    "        dropout: float = 0.1, \n",
    "        feed_forward_dimension: int = 2048\n",
    "    ) -> Transformer:\n",
    "    \n",
    "    source_embed = InputEmbeddings(model_dimension, source_vocab_size)\n",
    "    target_embed = InputEmbeddings(model_dimension, target_vocab_size)\n",
    "\n",
    "    source_pos = PositionalEncoding(model_dimension, source_context_size, dropout)\n",
    "    target_pos = PositionalEncoding(model_dimension, target_context_size, dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for _ in range(number_of_blocks):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(model_dimension, heads, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(model_dimension, feed_forward_dimension, dropout)\n",
    "        encoder_block = EncoderBlock(model_dimension, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    decoder_blocks = []\n",
    "    for _ in range(number_of_blocks):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(model_dimension, heads, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(model_dimension, heads, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(model_dimension, feed_forward_dimension, dropout)\n",
    "        decoder_block = DecoderBlock(model_dimension, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    encoder = Encoder(model_dimension, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(model_dimension, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    projection_layer = ProjectionLayer(model_dimension, target_vocab_size)\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, source_embed, target_embed, source_pos, target_pos, projection_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "        config, \n",
    "        source_vocab_size: int, \n",
    "        target_vocab_size: int\n",
    "    ) -> Transformer:\n",
    "    \n",
    "    model = build_transformer(\n",
    "        source_vocab_size = source_vocab_size, \n",
    "        target_vocab_size = target_vocab_size, \n",
    "        source_context_size = config['context_size'], \n",
    "        target_context_size = config['context_size'], \n",
    "        model_dimension = config['model_dimension']\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening\n",
    "\n",
    "Konačno, treba da vidimo kako treniramo model. Za početak, pretpostavljamo da imamo bazu podataka, u kojoj imamo praktično dve kolone, a redovi su popunjeni parom rečenica na dva jezika.\n",
    "\n",
    "*Learning rate* je hiperparametar koji kontroliše koliko *loss* funkcija utiče na promenu parametara. Međutim, ispostavilo se da njegovo smanjivanje tokom treninga pozitivno utiče na trening. To je i logično, jer learning rate predstavlja dužinu \"koraka\" za koji se funkcija spusti, pa što više treniramo, to smo bliže minimumu, i to manje moramo da se pomeramo da bismo ga \"pogodili\". Ovo se postiže korišćenjem optimizer-a, a to je u našem slučaju `torch.optim.Adam(model_parameters, learning_rate, ...)`.\n",
    "\n",
    "Postoji mogućnost čuvanja i učitavanja već treniranih weight-ova modela, ukoliko treniranje ide iz više koraka, ili ukoliko je potrebno da se pristupi modelu za inferencu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss funkcija\n",
    "\n",
    "*Loss* funkcija koju model koristi je Cross Entropy Loss.\n",
    "\n",
    "Videli smo malopre da je ulaz u decoder ***[SOS] BRZI VOZ 271 DOLAZI KOLOSEK 7 [PAD] ... [PAD]***, i da želimo da konačni izlaz iz transformer-a bude ***BRZI VOZ 271 DOLAZI KOLOSEK 7 [EOS] [PAD] ... [PAD]***. Ovo znači da će naša *loss* funkcija uporediti ovaj željeni izlaz sa dobijenim izlazom. Ova konkretna *loss* funkcija se računa po formuli:\n",
    "\n",
    "$$L(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{i=1}^{\\text{vocab size}} y_i \\log (\\hat{y}_i).$$\n",
    "\n",
    "Varijable u prošloj formuli su sledeće:\n",
    "\n",
    "1. $\\mathbf{y}$ predstavlja vektor dužine `vocab_size`, gde je jedinica na koordinati tačnog tokena, a sve ostalo nule.\n",
    "\n",
    "2. $\\hat{\\mathbf{y}}$ predstavlja izlaz iz modela u obliku verovatnoća za konkretan token.\n",
    "\n",
    "3. $y_i$ i $\\hat{y}_i$ su $i$-te koordinate ovih vektora.\n",
    "\n",
    "Konačno, kada saberemo ove formule primenjene na svaki vektor za ulaz i izlaz decoder-a, i rezultat podelimo sa njihovim brojem, dobijamo formulu:\n",
    "\n",
    "$$L(\\mathbf{Y}, \\hat{\\mathbf{Y}}) = -\\frac{1}{N} \\sum_{n = 1}^N \\sum_{i = 1}^{\\text{vocab size}} y_{n, i} \\log (\\hat{y}_{n, i}).$$\n",
    "\n",
    "U ovom slučaju $\\mathbf{Y}$ i $\\hat{\\mathbf{Y}}$ su matrice gde ima $N$ redova i $\\text{vocab size}$ kolona, a svaki red je odgovarajući token u ulazu, odnosno izlazu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dakle, tokom treninga stalno računamo vrednost *loss* funkcije, nakon računanja ažuriramo weight-ove tako da u sledećem koraku *loss* bude, nadamo se, smanjen. Weight-ovi se ažuriraju korišćenjem backpropagation-a, kao i u svakom ML modelu.\n",
    "\n",
    "Osim toga, svaki korak modela se sastoji iz jednog batch-a, što je način da model radi sa više ulaza odjednom, čime paralelizujemo treniranje i ubrzavamo model (ali ne linearno!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre nego što definišemo model, treba i da znamo koji fajl, u situaciji da postoji, treba da pročitamo da bismo dobili weight-ove modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(\n",
    "        config, \n",
    "        epoch: str\n",
    "    ) -> str:\n",
    "    model_folder = config['model_folder']\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "def get_latest_weights(config) -> str:\n",
    "\n",
    "    model_folder = config['model_folder']\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = f\"{model_basename}*\"\n",
    "    model_filenames = list(Path(model_folder).glob(model_filename))\n",
    "\n",
    "    if len(model_filenames) == 0:\n",
    "        return None\n",
    "    \n",
    "    def extract_epoch(filename):\n",
    "        return int(filename.stem.split('_')[-1])\n",
    "    \n",
    "    model_filenames.sort(key = extract_epoch)\n",
    "\n",
    "    return str(model_filenames[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takođe, moramo i bazu da dostavimo u odgovarajućoj formi. U našem slučaju, koristimo prvo funkciju `load_data(source_language, target_language)` da učitamo `{source_language}-{target_language}.tmx` fajl, a zatim funkciju `get_dataset(config)` da bismo definisali trening, test i validacijske skupove, kao i način za njihovo učitavanje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        source_language: str, \n",
    "        target_language: str\n",
    "    ) -> HFDataset:\n",
    "    with open(f\"{source_language}-{target_language}.tmx\", \"rb\") as fin:\n",
    "        tmx_file = tmxfile(fin, \"en\", \"sr_Cyrl\")\n",
    "\n",
    "    data = {'id' : [], 'translation': []}\n",
    "    i = 0\n",
    "\n",
    "    for item in tmx_file.unit_iter():\n",
    "\n",
    "        data[\"id\"].append(str(i))\n",
    "        i = i + 1\n",
    "\n",
    "        data[\"translation\"].append({f\"{source_language}\": item.source.strip('\"\\n').lower(), f\"{target_language}\": item.target.strip('\"\\n').lower()})\n",
    "\n",
    "    dataset = HFDataset.from_dict(data)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(config):\n",
    "    dataset_raw = load_data(config['source_language'], config['target_language'])\n",
    "\n",
    "    dataset_size = len(dataset_raw)\n",
    "    train_dataset_size = int(0.9 * dataset_size)\n",
    "    validation_dataset_size = int(0.08 * dataset_size)\n",
    "    test_dataset_size = dataset_size - train_dataset_size - validation_dataset_size\n",
    "\n",
    "    training_dataset_raw, validation_dataset_raw, test_dataset_raw = random_split(dataset_raw, [train_dataset_size, validation_dataset_size, test_dataset_size])\n",
    "\n",
    "    source_tokenizer = get_or_build_tokenizer(config, training_dataset_raw, config['source_language'], force_rewrite = True)\n",
    "    target_tokenizer = get_or_build_tokenizer(config, training_dataset_raw, config['target_language'], force_rewrite = True)\n",
    "\n",
    "    training_dataset = BilingualDataset(training_dataset_raw, source_tokenizer, target_tokenizer, config['source_language'], config['target_language'], config['context_size'])\n",
    "    validation_dataset = BilingualDataset(validation_dataset_raw, source_tokenizer, target_tokenizer, config['source_language'], config['target_language'], config['context_size'])\n",
    "    test_dataset = BilingualDataset(test_dataset_raw, source_tokenizer, target_tokenizer, config['source_language'], config['target_language'], config['context_size'])\n",
    "\n",
    "    training_dataloader = DataLoader(training_dataset, batch_size = config['batch_size'], shuffle = True)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size = 1, shuffle = True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = True)\n",
    "\n",
    "    return training_dataloader, validation_dataloader, test_dataloader, source_tokenizer, target_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device {device}.')\n",
    "\n",
    "    Path(config['model_folder']).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    training_dataloader, validation_dataloader, test_dataloader, source_tokenizer, target_tokenizer = get_dataset(config)\n",
    "    model = get_model(config, source_tokenizer.get_vocab_size(), target_tokenizer.get_vocab_size()).to(device)\n",
    "\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config['learning_rate'], eps = 1e-9)\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = get_latest_weights(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "    if model_filename:\n",
    "        print(f\"Preloading model {model_filename}.\")\n",
    "        state = torch.load(model_filename)\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print(\"No model to preload, starting from the beginning.\")\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = source_tokenizer.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        \n",
    "        batch_iterator = tqdm(training_dataloader, desc = f\"Processing epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            transformer_output = model.project(decoder_output)\n",
    "\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            loss = loss_function(transformer_output.view(-1, target_tokenizer.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(model, validation_dataloader, source_tokenizer, target_tokenizer, config['context_size'], device, lambda msg: batch_iterator.write(msg), writer, global_step, number_examples = 1)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        if epoch % 50 == 49 or epoch == 0 or epoch == config['num_epochs'] - 1:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step\n",
    "            }, model_filename)\n",
    "            \n",
    "    run_validation(model, validation_dataloader, source_tokenizer, target_tokenizer, config['context_size'], device, lambda msg: batch_iterator.write(msg), writer, global_step, number_examples = 50)\n",
    "    run_test(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U ovom kodu:\n",
    "\n",
    "1. `writer` je način za vizualizaciju podataka. Njega ćemo videti ponovo u validaciji.\n",
    "\n",
    "2. `loss.backward()` računa izvode i zapisuje ih u sve parametre za koje je `require_grad = True`, a onda `optimizer.step()` ažurira vrednosti tih parametara.\n",
    "\n",
    "3. `run_validation` i `run_test` su funkcije koje ćemo kasnije implementirati, a koje obavljaju validaciju i testiranje modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacija\n",
    "\n",
    "Validacija generalno služi za podešavanje hiperparametara modela. \n",
    "\n",
    "U slučaju transformera, validacija radi na sledeći način. Pretpostavimo da imamo rečenicu ***Voz za Kraljevo kasni.*** nad kojom hoćemo da izvršimo validaciju. Neka je njen tačan prevod ***VOZ DO KRALJEVO KASNI***.\n",
    "\n",
    "1. Izvršimo encoder i dobijemo odgovarajući izlaz za datu rečenicu.\n",
    "\n",
    "2. U decoder ubacujemo samo ***[SOS]***. Izlaz treba da bude neki token $T_1^1$.\n",
    "\n",
    "3. U decoder ubacujemo ***[SOS] VOZ***. Izlaz treba da budu dva tokena ***VOZ*** $T_2^2$.\n",
    "\n",
    "4. U decoder ubacujemo ***[SOS] VOZ DO***. Izlaz treba da budu tri tokena ***VOZ DO*** $T_3^3$.\n",
    "\n",
    "5. Ovaj proces ponavljamo dok ne prođemo celu rečenicu.\n",
    "\n",
    "Dakle, validiramo kao da imamo više malih rečenica, i uvek predviđamo po tačno $1$ sledeći token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zato su nam potrebne dve funkcije, a to su:\n",
    "\n",
    "1. Predviđanje sledećeg tokena, na osnovu treniranog modela i izlaza iz encoder-a - `_greedy_decode_next_token(model, encoder_output, decoder_input, source_mask, device)`;\n",
    "\n",
    "2. Funkcija `run_validation(model, validation_dataset, source_tokenizer, target_tokenizer, max_length, device, print_msg, writer, global_step, number_examples)` od malopre, koja služi da bismo tokom treniranja mogli da radimo validaciju modela.\n",
    "\n",
    "Takođe, definisaćemo i funkciju `_greedy_decode(model, encoder_input, source_mask, source_tokenizer, target_tokenizer, max_length, device)`, koja služi da predvidi ceo izlaz, počevši sa tokenom $\\left[\\text{SOS}\\right]$. Tokom testiranja ćemo videti zašto je ona bitna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _greedy_decode(\n",
    "        model: Transformer, \n",
    "        encoder_input: torch.Tensor, \n",
    "        source_mask: torch.Tensor, \n",
    "        source_tokenizer: Tokenizer, \n",
    "        target_tokenizer: Tokenizer, \n",
    "        max_length: int, \n",
    "        device: str\n",
    "    ) -> torch.Tensor:\n",
    "    sos_index = target_tokenizer.token_to_id('[SOS]')\n",
    "    eos_index = target_tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(encoder_input, source_mask)\n",
    "\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_index).type_as(encoder_input).to(device)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if decoder_input.size(1) == max_length:\n",
    "            break\n",
    "\n",
    "        decoder_input, next_token = _greedy_decode_next_token(model, encoder_output, decoder_input, source_mask, device)\n",
    "\n",
    "        if next_token == eos_index:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def _greedy_decode_next_token(\n",
    "        model: Transformer, \n",
    "        encoder_output: torch.Tensor,\n",
    "        decoder_input: torch.Tensor,\n",
    "        source_mask: torch.Tensor, \n",
    "        device: str\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "    out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "    prob = model.project(out[:, -1])\n",
    "    _, next_token = torch.max(prob, dim = 1)\n",
    "\n",
    "    decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(decoder_input).fill_(next_token.item()).to(device)], dim = 1)\n",
    "\n",
    "    return decoder_input, next_token\n",
    "\n",
    "\n",
    "def run_validation(\n",
    "        model: Transformer, \n",
    "        validation_dataset: DataLoader, \n",
    "        source_tokenizer: Tokenizer, \n",
    "        target_tokenizer: Tokenizer, \n",
    "        max_length: int, \n",
    "        device: str, \n",
    "        print_msg, \n",
    "        writer: SummaryWriter, \n",
    "        global_step: int, \n",
    "        number_examples: int = 2\n",
    "    ) -> None:\n",
    "    model.eval()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        with os.open(\"stty size\", \"r\") as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in validation_dataset:\n",
    "\n",
    "            source_text = batch['source_text'][0]\n",
    "            print_msg('-' * console_width)\n",
    "            print_msg(f\"Source: {source_text}\")\n",
    "\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            sos_index = target_tokenizer.token_to_id('[SOS]')\n",
    "\n",
    "            target_ids = target_tokenizer.encode(batch['target_text'][0]).ids\n",
    "\n",
    "            next_token = None\n",
    "\n",
    "            decoder_input_slice = torch.empty(1, 1).fill_(sos_index).type_as(encoder_input).to(device)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "\n",
    "            next_tokens_predicted = []\n",
    "            next_tokens_actual = []\n",
    "\n",
    "            i = 1\n",
    "            while i <= len(target_ids):\n",
    "\n",
    "                _, next_token = _greedy_decode_next_token(model, encoder_output, decoder_input_slice, encoder_mask, device)\n",
    "\n",
    "                next_token = next_token.squeeze(0).detach().cpu().numpy()\n",
    "                if next_token.ndim == 0:\n",
    "                    next_token = [next_token.item()]\n",
    "                elif next_token.ndim == 1:\n",
    "                    next_token = next_token.tolist()\n",
    "\n",
    "                next_tokens_predicted.append(target_tokenizer.decode(next_token))\n",
    "                next_tokens_actual.append(target_tokenizer.decode([torch.tensor(target_ids[i - 1]).unsqueeze(0).to(device).squeeze(0).detach().cpu().numpy()]))\n",
    "\n",
    "                decoder_input_slice = torch.tensor(target_ids[0 : i]).unsqueeze(0).to(device)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            table = [next_tokens_predicted, next_tokens_actual]\n",
    "            print_msg(tabulate(table, headers = 'keys', showindex = True, tablefmt = 'grid'))\n",
    "\n",
    "            model_out = _greedy_decode(model, encoder_input, encoder_mask, source_tokenizer, target_tokenizer, max_length, device)\n",
    "\n",
    "            target_text = batch['target_text'][0]\n",
    "\n",
    "            model_out_text = target_tokenizer.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            print_msg(f\"Target: {target_text}\")\n",
    "            print_msg(f\"Predicted: {model_out_text}\")\n",
    "\n",
    "            if count == number_examples:\n",
    "                print_msg('-' * console_width)\n",
    "                break\n",
    "\n",
    "    if writer:\n",
    "\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testiranje\n",
    "\n",
    "Konačno, da bismo testirali model, treba da mu na neki način dozvolimo da generiše celu rečenicu.\n",
    "\n",
    "To postižemo na sličan način kao u validaciji. Pretpostavljamo opet isti ulaz *Voz za Kraljevo kasni.* Model u početku ne zna za ovu rečenicu, nije bila u trening skupu, pa nemamo nikakav unapred poznat ulaz u decoder.\n",
    "\n",
    "1. Izvršimo encoder i dobijemo odgovarajući izlaz za datu rečenicu.\n",
    "\n",
    "2. U decoder ubacujemo samo $\\left[\\text{SOS}\\right]$. Izlaz treba da bude neki token $T_1$.\n",
    "\n",
    "3. U decoder ubacujemo $\\left[\\text{SOS}\\right]\\ T_1$. Izlaz treba da budu dva tokena $T_1\\ T_2$.\n",
    "\n",
    "4. U decoder ubacujemo $\\left[\\text{SOS}\\right]\\ T_1\\ T_2$. Izlaz treba da budu tri tokena $T_1\\ T_2\\ T_3$.\n",
    "\n",
    "5. Ovaj proces ponavljamo dok ne dođemo do izlaza $T_1\\ T_2\\ T_3\\ \\dots\\ T_n\\ \\left[\\text{EOS}\\right]$.\n",
    "\n",
    "Dakle, model generiše jedan po jedan token, ažurirajući svoj ulaz u decoder, sve dok ne dobijemo token $\\left[\\text{EOS}\\right]$ u izlazu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bismo to uradili, potrebno nam je još funkcija:\n",
    "\n",
    "1. `_prepare_model(config)` koja priprema sve neophodne karakteristike modela;\n",
    "\n",
    "2. `_translate_sentence(model, sentence, source_tokenizer, target_tokenizer, max_length, device, pad_token, predict_next_tokens)` koja sa datim karakteristikama modela prevodi zadatu rečenicu;\n",
    "\n",
    "3. `translate_sentence(sentence)` koja je user-friendly wrapper za funkciju `_translate_sentence` (jer kao ulaz uzima samo string koji predstavlja rečenicu koju treba prevesti);\n",
    "\n",
    "4. `translate_sentences(sentences)` koja prevodi listu rečenica, na sličan način kao i prethodna funkcija;\n",
    "\n",
    "5. `run_test(test_dataset)` koja prevodi rečenice iz test baze (koja je odvojena pre treniranja modela)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_model(config):\n",
    "    source_language = config['source_language']\n",
    "    target_language = config['target_language']\n",
    "\n",
    "    source_tokenizer = Tokenizer.from_file(str(config['tokenizer_file'].format(source_language)))\n",
    "    target_tokenizer = Tokenizer.from_file(str(config['tokenizer_file'].format(target_language)))\n",
    "\n",
    "    max_length = config['context_size']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = get_model(config, source_tokenizer.get_vocab_size(), target_tokenizer.get_vocab_size()).to(device)\n",
    "    model_filename = get_latest_weights(config)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "    pad_token = torch.tensor([source_tokenizer.token_to_id('[PAD]')], dtype = torch.int64).to(device)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'source_tokenizer': source_tokenizer,\n",
    "        'target_tokenizer': target_tokenizer,\n",
    "        'max_length': max_length,\n",
    "        'device': device,\n",
    "        'pad_token': pad_token\n",
    "    }\n",
    "\n",
    "def _translate_sentence(\n",
    "        model: Transformer,\n",
    "        sentence: str,\n",
    "        source_tokenizer: Tokenizer,\n",
    "        target_tokenizer: Tokenizer,\n",
    "        max_length: int,\n",
    "        device: str,\n",
    "        pad_token: torch.Tensor,\n",
    "        predict_next_tokens: Callable[[Transformer, torch.Tensor, torch.Tensor, Tokenizer, Tokenizer, int, str], torch.Tensor] = _greedy_decode\n",
    "    ) -> str:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        source_ids = source_tokenizer.encode(sentence).ids\n",
    "        encoder_input = torch.tensor(source_ids).unsqueeze(0).to(device)\n",
    "\n",
    "        source_mask = (encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "\n",
    "        model_output = predict_next_tokens(\n",
    "            model,\n",
    "            encoder_input,\n",
    "            source_mask,\n",
    "            source_tokenizer,\n",
    "            target_tokenizer,\n",
    "            max_length,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        translated_sentence = target_tokenizer.decode(model_output.detach().cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return translated_sentence\n",
    "\n",
    "def translate_sentence(sentence: str) -> str:\n",
    "    config = get_config()\n",
    "\n",
    "    model_parameters = _prepare_model(config)\n",
    "\n",
    "    model = model_parameters['model']\n",
    "    source_tokenizer = model_parameters['source_tokenizer']\n",
    "    target_tokenizer = model_parameters['target_tokenizer']\n",
    "    max_length = model_parameters['max_length']\n",
    "    device = model_parameters['device']\n",
    "    pad_token = model_parameters['pad_token']\n",
    "\n",
    "    translation = _translate_sentence(\n",
    "        model,\n",
    "        sentence,\n",
    "        source_tokenizer,\n",
    "        target_tokenizer,\n",
    "        max_length,\n",
    "        device,\n",
    "        pad_token\n",
    "    )\n",
    "\n",
    "    return translation\n",
    "\n",
    "def translate_sentences(sentences: List[str]) -> List[str]:\n",
    "    config = get_config()\n",
    "\n",
    "    model_parameters = _prepare_model(config)\n",
    "\n",
    "    model = model_parameters['model']\n",
    "    source_tokenizer = model_parameters['source_tokenizer']\n",
    "    target_tokenizer = model_parameters['target_tokenizer']\n",
    "    max_length = model_parameters['max_length']\n",
    "    device = model_parameters['device']\n",
    "    pad_token = model_parameters['pad_token']\n",
    "\n",
    "    translations = []\n",
    "    for sentence in sentences:\n",
    "        translations.append((sentence, _translate_sentence(\n",
    "            model,\n",
    "            sentence,\n",
    "            source_tokenizer,\n",
    "            target_tokenizer,\n",
    "            max_length,\n",
    "            device,\n",
    "            pad_token\n",
    "            )))\n",
    "        \n",
    "    return translations\n",
    "\n",
    "\n",
    "def run_test(test_dataset: DataLoader):\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    for batch in test_dataset:\n",
    "\n",
    "        source_text = batch['source_text'][0]\n",
    "        sentences.append(source_text)\n",
    "\n",
    "    translations = translate_sentences(sentences)\n",
    "    for translation in translations:\n",
    "        print(f\"Original: {translation[0]}\")\n",
    "        print(f\"Translated: {translation[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
